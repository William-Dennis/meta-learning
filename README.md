# Meta-Learning using Simulated Annealing

This repository demonstrates using **Proximal Policy Optimization (PPO)** to automatically tune **Simulated Annealing (SA)** hyperparameters for minimizing the 2D Rastrigin function.

## ğŸš€ Quick Start

### Prerequisites
- Python 3.12+
- UV package manager

### Installation

```bash
# Install UV
pip install uv

# Install dependencies
uv venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
uv pip install numpy matplotlib torch maturin

# Build Rust extension (optional, for better performance)
maturin develop --release
```

### Running Experiments

```bash
# Run PPO training to tune SA hyperparameters
python run_experiment.py

# Run grid search over SA hyperparameters
python run_grid_search.py
```

## ğŸ“ Repository Structure

```
meta-learning/
â”œâ”€â”€ run_experiment.py      # PPO training runner
â”œâ”€â”€ run_grid_search.py     # Grid search runner
â”œâ”€â”€ core/                  # Core implementation modules
â”‚   â”œâ”€â”€ sa_algorithms/     # SA algorithm implementations
â”‚   â”‚   â”œâ”€â”€ python_serial.py   # Python serial SA
â”‚   â”‚   â””â”€â”€ rust_parallel.py   # Rust parallel SA (fast)
â”‚   â”œâ”€â”€ sa_config.py       # SA algorithm configuration
â”‚   â”œâ”€â”€ tuning_env.py      # PPO training environment
â”‚   â””â”€â”€ ppo_agent.py       # PPO agent implementation
â”œâ”€â”€ outputs/               # Generated plots and results
â”œâ”€â”€ src/                   # Rust source code
â”‚   â””â”€â”€ lib.rs            # Rust SA implementation
â””â”€â”€ README.md             # This file
```

## ğŸ¯ Algorithm Selection

The repository includes two SA implementations:
- **python_serial**: Pure Python implementation (baseline)
- **rust_parallel**: Rust parallel implementation (recommended for speed)

To switch algorithms, edit `core/sa_config.py`:

```python
ALGORITHM = 'rust_parallel'  # or 'python_serial'
```

## ğŸ”‘ Key Features

- âœ… Simple, clean codebase
- âœ… PPO-based hyperparameter tuning
- âœ… Rust acceleration for performance
- âœ… Automatic output organization
- âœ… Seed-based reproducibility
- âœ… All functions â‰¤ 30 lines

## ğŸ“Š Outputs

All generated outputs are saved in the `outputs/` directory:
- Training curves
- Parameter evolution plots
- Trajectory visualizations
- Performance metrics

## ğŸ› ï¸ Development

**Always use UV for dependency management:**

```bash
# Add a new dependency
uv pip install <package>

# Never use pip directly except to install UV itself
```

## ğŸ“ Notes

- All randomness is controlled via the `seed` parameter
- Functions are kept simple (â‰¤ 30 lines)
- Code follows clean, minimal design principles

## ğŸ“„ License

See [LICENSE](LICENSE) file.
